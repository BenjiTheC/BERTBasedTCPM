{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Notebook for final model building evaluation\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, scale, normalize\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_predict,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    KFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from tc_data import TopCoder\n",
    "from final_model_selection import (\n",
    "    kfold_predict_validate_gradient_boosting,\n",
    "    kfold_predict_validate_neural_network,\n",
    "    train_gb_for_production\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option('display.max_rows', 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data object\n",
    "\n",
    "`TopCoder` class contain the pre-processed data retrieved from data base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tc = TopCoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = tc.build_final_dataset('avg_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Gradient Boosting against NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_dct = {\n",
    "    'avg_score': {'gb': {}, 'nn': {}},\n",
    "    'number_of_registration': {'gb': dict(tol=4), 'nn': dict(es_min_delta=4)},\n",
    "    'sub_reg_ratio': {'gb': dict(loss='lad', tol=0.001, n_iter_no_change=10), 'nn': dict(es_min_delta=0.001)},\n",
    "    'total_prize': {'gb': dict(loss='ls', tol=10, n_iter_no_change=5), 'nn': dict(es_min_delta=10)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for target, param_dct in hyper_param_dct.items():\n",
    "    X, y = tc.build_final_dataset(target)\n",
    "    gb_y_pred, gb_cv_eval_df, gb_manual_score, gb_cv_fi = kfold_predict_validate_gradient_boosting(X, y, **param_dct['gb'])\n",
    "    nn_y_pred, nn_cv_eval_df, nn_manual_score = kfold_predict_validate_neural_network(X, y, **param_dct['nn'])\n",
    "    \n",
    "    res[target] = dict(\n",
    "        y=y,\n",
    "        gb_y_pred=gb_y_pred,\n",
    "        nn_y_pred=nn_y_pred,\n",
    "        gb_cv_mean=gb_cv_eval_df.mean(),\n",
    "        nn_cv_mean=nn_cv_eval_df.mean(),\n",
    "        gb_manual=pd.Series(gb_manual_score),\n",
    "        nn_manual=pd.Series(nn_manual_score)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_idx_dct = defaultdict(dict)\n",
    "algo_name = {\n",
    "    'gb': 'Gradient Boosting',\n",
    "    'nn': 'Neural Network',\n",
    "}\n",
    "result_name = {\n",
    "    'cv_mean': 'CV Mean',\n",
    "    'manual': 'Full Set',\n",
    "}\n",
    "\n",
    "for target in res:\n",
    "    for algo in ('gb', 'nn'):\n",
    "        for result in ('cv_mean', 'manual'):\n",
    "            for metric, score in res[target][f'{algo}_{result}'].iteritems():\n",
    "                multi_idx_dct[(metric, result_name[result])][(target, algo_name[algo])] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_metrics = pd.DataFrame.from_dict(multi_idx_dct, orient='index').sort_index()\n",
    "prediction_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score_df = pd.read_json('./baseline/avg_score_result.json', orient='index')\n",
    "number_of_registration_df = pd.read_json('./baseline/number_of_registration_result.json', orient='index')\n",
    "sub_reg_ratio_df = pd.read_json('./baseline/sub_reg_ratio_result.json', orient='index')\n",
    "total_prize_df = pd.read_json('./baseline/total_prize_result.json', orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(res['avg_score']['gb_y_pred'].index == avg_score_df.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind, ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(res['avg_score']['gb_y_pred'], avg_score_df['avg_score_pred'], equal_var=False).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(res['number_of_registration']['gb_y_pred'], number_of_registration_df['number_of_registration_pred'], equal_var=False).pvalue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(res['sub_reg_ratio']['gb_y_pred'], sub_reg_ratio_df['sub_reg_ratio_pred'], equal_var=False).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttest_ind(res['total_prize']['gb_y_pred'], total_prize_df['total_prize_pred'], equal_var=False).pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "endpoint = [100, 300, 1, 2500]\n",
    "\n",
    "with sns.axes_style('darkgrid'):\n",
    "    fig, axes = plt.subplots(4, 2, figsize=(8, 16), dpi=200)\n",
    "    for i, (target, predict_res) in enumerate(res.items()):\n",
    "        for j, (algo, title) in enumerate(algo_name.items()):\n",
    "            ax = axes[i, j]\n",
    "            sns.scatterplot(\n",
    "                x=predict_res[f'{algo}_y_pred'],\n",
    "                y=predict_res['y'],\n",
    "                s=20,\n",
    "                alpha=0.5,\n",
    "                linewidth=0.1,\n",
    "                ax=ax,\n",
    "            )\n",
    "            sns.lineplot(\n",
    "                x=[0, endpoint[i]],\n",
    "                y=[0, endpoint[i]],\n",
    "                color='red',\n",
    "                alpha=0.75,\n",
    "                ax=ax,\n",
    "            )\n",
    "            ax.set_title(f'{target.capitalize()} - {title}')\n",
    "            ax.set_xlabel('Prediction')\n",
    "            ax.set_ylabel('Ground Truth')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "#     fig.savefig('./result/img/pred_against_truth.png', dpi='figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the feature importance of Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_dct = {\n",
    "    'avg_score': {},\n",
    "    'number_of_registration': dict(tol=4),\n",
    "    'sub_reg_ratio': dict(loss='lad', tol=0.001, n_iter_no_change=10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_res = {}\n",
    "for target, param in gb_param_dct.items():\n",
    "    X, y = tc.build_final_dataset(target)\n",
    "    y_pred, cv_eval_df, manual_score, cv_feature_importance = kfold_predict_validate_gradient_boosting(X, y, **param)\n",
    "\n",
    "    gb_res[target] = dict(\n",
    "        y=y,\n",
    "        y_pred=y_pred,\n",
    "        cv_mean=cv_eval_df.mean(),\n",
    "        manual=pd.Series(manual_score),\n",
    "        fea_importance = cv_feature_importance.mean()\n",
    "    )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midx_dct = defaultdict(dict)\n",
    "for target, res_dct in gb_res.items():\n",
    "    for result in 'cv_mean', 'manual':\n",
    "        for metric, score in res_dct[result].items():\n",
    "            midx_dct[target][(metric, result)] = score\n",
    "        \n",
    "res_df = pd.DataFrame.from_dict(midx_dct).sort_index()\n",
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "            |approach|baseline|\n",
    "            |.  |.  |.  ||.  |. |   |\n",
    "    metric0|\n",
    "    metric1|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_df.to_json('result/final_models/extened_features_gradient_boosting_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_imp_dct = {\n",
    "    target: res_dct['fea_importance'] for target, res_dct in gb_res.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [f'#{color}' for color in'641220-6e1423-85182a-a11d33-a71e34-b21e35-bd1f36-c71f37-da1e37-e01e37'.split('-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style('darkgrid'):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 8), dpi=200, sharex=True)\n",
    "    for i, (target, feature_imp) in enumerate(fea_imp_dct.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        sns.barplot(\n",
    "            x=feature_imp.sort_values(ascending=False).head(10),\n",
    "            y=feature_imp.sort_values(ascending=False).head(10).index,\n",
    "            ax=ax,\n",
    "            palette=palette[::-1]\n",
    "        )\n",
    "        ax.set_title('{} - Feature Importance'.format(' '.join([w.capitalize() for w in target.split('_')])))\n",
    "        ax.set_xlim(0, 1)\n",
    "        \n",
    "        for p in ax.patches:\n",
    "            x = p.get_width() + 0.01\n",
    "            y = p.get_y() + 0.5 * p.get_height() + 0.002\n",
    "            importance = round(p.get_width(), 3)\n",
    "            ax.text(x, y, importance, va='center', fontdict={'fontsize': 8})\n",
    "            \n",
    "    fig.tight_layout()\n",
    "#     fig.savefig('result/img/gradient_boosting_feature_importance.png', dpi='figure')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model using full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_dct = {\n",
    "    'avg_score': dict(),\n",
    "    'number_of_registration': dict(tol=4),\n",
    "    'sub_reg_ratio': dict(loss='lad', tol=0.001, n_iter_no_change=10),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimators_dct = {}\n",
    "for target, param in gb_param_dct.items():\n",
    "    print(f'\\nTraining target: {target}...')\n",
    "    X, y = tc.build_final_dataset(target)\n",
    "    estimators_dct[target] = train_gb_for_production(X, y, target, **param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [f'#{color}' for color in'641220-6e1423-85182a-a11d33-a71e34-b21e35-bd1f36-c71f37-da1e37-e01e37'.split('-')]\n",
    "with sns.axes_style('darkgrid'):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 8), dpi=200, sharex=True)\n",
    "    for i, (target, pipeline) in enumerate(estimators_dct.items()):\n",
    "        ax = axes[i]\n",
    "        feature_imp = pd.Series(pipeline[f'{target}_gb_reg'].feature_importances_, index=X.columns)\n",
    "        sns.barplot(\n",
    "            x=feature_imp.sort_values(ascending=False).head(10),\n",
    "            y=feature_imp.sort_values(ascending=False).head(10).index,\n",
    "            ax=ax,\n",
    "            palette=palette[::-1]\n",
    "        )\n",
    "        ax.set_title('{} - Feature Importance'.format(' '.join([w.capitalize() for w in target.split('_')])))\n",
    "        ax.set_xlim(0, 1)\n",
    "        \n",
    "        for p in ax.patches:\n",
    "            x = p.get_width() + 0.01\n",
    "            y = p.get_y() + 0.5 * p.get_height() + 0.002\n",
    "            importance = round(p.get_width(), 3)\n",
    "            ax.text(x, y, importance, va='center', fontdict={'fontsize': 8})\n",
    "            \n",
    "    fig.tight_layout()\n",
    "#     fig.savefig('result/img/gb_feature_imp_train_with_whole_data.png', dpi='figure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = X.sample(1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lst = []\n",
    "for prize in range(100, 2500):\n",
    "    s = sample.squeeze().copy()\n",
    "    s['total_prize'] = prize\n",
    "    sample_lst.append(s)\n",
    "\n",
    "sample_df = pd.DataFrame.from_records(sample_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['total_prize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dct = {}\n",
    "for target, gb_reg in estimators_dct.items():\n",
    "    y_pred_dct[target] = gb_reg.predict(sample_df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "palette = [f'#{color}' for color in'641220-6e1423-85182a-a11d33-a71e34-b21e35-bd1f36-c71f37-da1e37-e01e37'.split('-')]\n",
    "with sns.axes_style('darkgrid'):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(8, 8), dpi=200)\n",
    "    for i, (target, y_pred) in enumerate(y_pred_dct.items()):\n",
    "        ax = axes[i]\n",
    "\n",
    "        sns.lineplot(x=sample_df['total_prize'], y=y_pred, color='orange', ax=ax)\n",
    "        ax.set_xticks(list(range(100, 2600, 100)))\n",
    "        ax.set_ylabel(target)\n",
    "        ax.set_title(f'{target} prediction by posted prize')\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find sample case for background session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cha_info = tc.get_filtered_challenge_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cha_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expensive_cha = cha_info.loc[cha_info.total_prize >= 750]\n",
    "cheap_cha = cha_info.loc[cha_info.total_prize <= 500]\n",
    "\n",
    "while True:\n",
    "    exp_sample = expensive_cha.sample(1).squeeze()\n",
    "    cheap_sample = cheap_cha.sample(1).squeeze()\n",
    "    \n",
    "    score_diff = exp_sample['avg_score'] - cheap_sample['avg_score']\n",
    "    reg_diff = exp_sample['number_of_registration'] - cheap_sample['number_of_registration']\n",
    "    sub_diff = exp_sample['sub_reg_ratio'] - cheap_sample['sub_reg_ratio']\n",
    "    dura_diff = exp_sample['challenge_duration'] - cheap_sample['challenge_duration']\n",
    "    \n",
    "    if (\n",
    "        exp_sample['avg_score'] > 90 and \n",
    "        exp_sample['number_of_submitters'] > 1 and\n",
    "        cheap_sample['number_of_submitters'] > 1 and\n",
    "        score_diff >= 3 and \n",
    "        reg_diff > 0 and \n",
    "        sub_diff > 0 and \n",
    "        dura_diff == 0\n",
    "    ):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([exp_sample, cheap_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([exp_sample, cheap_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([exp_sample, cheap_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([exp_sample, cheap_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([exp_sample, cheap_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([exp_sample, cheap_sample], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (BERT TCPM)",
   "language": "python",
   "name": "bert_tcpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
